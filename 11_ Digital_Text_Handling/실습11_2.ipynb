{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thermal-jewel",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 학습셋 입력: 유시민 검찰도 사법도 썩었지 vs 진중권 유시민 망상 대중은 현실로 믿어\n",
      "첫번째 테스트셋 입력: 박준영 “도자기 집에서 사용” 김선교 “궁궐 살았나”\n",
      "첫번째 학습셋 결과 one-hot 출력[1. 0. 0. 0.]\n",
      "첫번째 테스트셋 결과 one-hot 출력[1. 0. 0. 0.]\n",
      "전체 데이터셋 단어 토큰 개수: 23319\n",
      "첫번째 학습셋 토큰 결과: [243, 7314, 7315, 7316, 122, 413, 243, 7317, 7318, 4360, 4361]\n",
      "첫번째 테스트셋 토큰 결과: [3139, 22912, 1069, 22913, 22914, 22915, 22916]\n",
      "학습셋 제목 최대 길이: 18\n",
      "테스트셋 제목 최대 길이: 14\n",
      "첫번째 패딩 토큰: [   0    0    0    0    0    0    0  243 7314 7315 7316  122  413  243\n",
      " 7317 7318 4360 4361]\n",
      "첫번째 패딩 토큰 토큰[    0     0     0     0     0     0     0     0     0     0     0  3139\n",
      " 22912  1069 22913 22914 22915 22916]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 18)          419760    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 18)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          5824      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 55)                26400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 224       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 452,208\n",
      "Trainable params: 452,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shbk0\\AppData\\Local\\conda\\conda\\envs\\tutorial\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 150 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 3s 275us/step - loss: 1.3734 - accuracy: 0.2989 - val_loss: 1.4129 - val_accuracy: 0.2400\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 207us/step - loss: 1.0635 - accuracy: 0.5008 - val_loss: 1.3105 - val_accuracy: 0.3333\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 180us/step - loss: 0.7299 - accuracy: 0.6917 - val_loss: 1.3549 - val_accuracy: 0.4467\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 186us/step - loss: 0.5306 - accuracy: 0.7943 - val_loss: 1.5144 - val_accuracy: 0.4000\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 191us/step - loss: 0.3993 - accuracy: 0.8509 - val_loss: 1.7082 - val_accuracy: 0.4067\n",
      "150/150 [==============================] - 0s 60us/step\n",
      "\n",
      " Test Accuracy: 0.4067\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 코드 내부에 한글을 사용가능 하게 해주는 부분입니다.\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = pd.read_csv('../dataset/train_SectionDataset.csv', delimiter=\",\")\n",
    "docs = data.title\n",
    "classes = np_utils.to_categorical(data.section)\n",
    "\n",
    "print('첫번째 학습셋 입력: ' + docs[0])\n",
    "\n",
    "test = pd.read_csv('../dataset/test_SectionDataset.csv', delimiter=\",\")\n",
    "test_docs = test.title\n",
    "test_classes = np_utils.to_categorical(test.section)\n",
    "\n",
    "print('첫번째 테스트셋 입력: ' + test_docs[0])\n",
    "print('첫번째 학습셋 결과 one-hot 출력' + str(classes[0]))\n",
    "print('첫번째 테스트셋 결과 one-hot 출력' + str(test_classes[0]))\n",
    "\n",
    "doc_append = docs.append(test_docs) #X_train + X_test 수행\n",
    "\n",
    "\n",
    "\n",
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(doc_append) #인덱스 부여\n",
    "x = token.texts_to_sequences(docs)\n",
    "x_test = token.texts_to_sequences(test_docs)\n",
    "\n",
    "word_size = len(token.word_index)\n",
    "print('전체 데이터셋 단어 토큰 개수: ' + str(word_size))\n",
    "\n",
    "print('첫번째 학습셋 토큰 결과: ' + str(x[0]))\n",
    "\n",
    "print('첫번째 테스트셋 토큰 결과: ' + str(x_test[0]))\n",
    "\n",
    "max_title = -1\n",
    "for i in range(len(x)):\n",
    "    if max_title < len(x[i]):\n",
    "        max_title = len(x[i])\n",
    "       \n",
    "print('학습셋 제목 최대 길이: ' + str(max_title))\n",
    "\n",
    "max_test_title = -1\n",
    "for j in range(len(x_test)):\n",
    "    if max_test_title < len(x_test[j]):\n",
    "        max_test_title = len(x_test[j])\n",
    "               \n",
    "print('테스트셋 제목 최대 길이: ' + str(max_test_title))\n",
    "\n",
    "#패딩\n",
    "padded_x = sequence.pad_sequences(x, maxlen=18)\n",
    "padded_test_x = sequence.pad_sequences(x_test, maxlen=18)\n",
    "\n",
    "print('첫번째 패딩 토큰: ' + str(padded_x[0]))\n",
    "print('첫번째 패딩 토큰 토큰' + str(padded_test_x[0]))\n",
    "\n",
    "# 모델의 설정\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size + 1, 18))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 5, padding='valid', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# 모델의 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델의 실행\n",
    "history = model.fit(padded_x, classes, batch_size=100, epochs=5, validation_data=(padded_test_x, test_classes))\n",
    "\n",
    "# 테스트 정확도 출력\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(padded_test_x, test_classes)[1]))\n",
    "\n",
    "\n",
    "# 테스트 셋의 오차\n",
    "#y_vloss = history.history['val_loss']\n",
    "\n",
    "# 학습셋의 오차\n",
    "#y_loss = history.history['loss']\n",
    "\n",
    "# 그래프로 표현\n",
    "#x_len = numpy.arange(len(y_loss))\n",
    "#plt.plot(x_len, y_vloss, marker='.', c=\"red\", label='Testset_loss')\n",
    "#plt.plot(x_len, y_loss, marker='.', c=\"blue\", label='Trainset_loss')\n",
    "\n",
    "# 그래프에 그리드를 주고 레이블을 표시\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.grid()\n",
    "#plt.xlabel('epoch')\n",
    "#plt.ylabel('loss')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "placed-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 학습셋 입력: 유시민 검찰도 사법도 썩었지 vs 진중권 유시민 망상 대중은 현실로 믿어\n",
      "첫번째 테스트셋 입력: 박준영 “도자기 집에서 사용” 김선교 “궁궐 살았나”\n",
      "첫번째 학습셋 결과 one-hot 출력[1. 0. 0. 0.]\n",
      "첫번째 테스트셋 결과 one-hot 출력[1. 0. 0. 0.]\n",
      "전체 데이터셋 단어 토큰 개수: 23319\n",
      "첫번째 학습셋 토큰 결과: [243, 7314, 7315, 7316, 122, 413, 243, 7317, 7318, 4360, 4361]\n",
      "첫번째 테스트셋 토큰 결과: [3139, 22912, 1069, 22913, 22914, 22915, 22916]\n",
      "학습셋 제목 최대 길이: 18\n",
      "테스트셋 제목 최대 길이: 14\n",
      "첫번째 패딩 토큰: [   0    0    0    0    0    0    0  243 7314 7315 7316  122  413  243\n",
      " 7317 7318 4360 4361]\n",
      "첫번째 패딩 토큰 토큰[    0     0     0     0     0     0     0     0     0     0     0  3139\n",
      " 22912  1069 22913 22914 22915 22916]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = pd.read_csv('../dataset/train_SectionDataset.csv', delimiter=\",\")\n",
    "docs = data.title\n",
    "classes = np_utils.to_categorical(data.section)\n",
    "\n",
    "print('첫번째 학습셋 입력: ' + docs[0])\n",
    "\n",
    "test = pd.read_csv('../dataset/test_SectionDataset.csv', delimiter=\",\")\n",
    "test_docs = test.title\n",
    "test_classes = np_utils.to_categorical(test.section)\n",
    "\n",
    "print('첫번째 테스트셋 입력: ' + test_docs[0])\n",
    "print('첫번째 학습셋 결과 one-hot 출력' + str(classes[0]))\n",
    "print('첫번째 테스트셋 결과 one-hot 출력' + str(test_classes[0]))\n",
    "\n",
    "doc_append = docs.append(test_docs) #X_train + X_test 수행\n",
    "\n",
    "\n",
    "\n",
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(doc_append) #인덱스 부여\n",
    "x = token.texts_to_sequences(docs)\n",
    "x_test = token.texts_to_sequences(test_docs)\n",
    "\n",
    "word_size = len(token.word_index)\n",
    "print('전체 데이터셋 단어 토큰 개수: ' + str(word_size))\n",
    "\n",
    "print('첫번째 학습셋 토큰 결과: ' + str(x[0]))\n",
    "\n",
    "print('첫번째 테스트셋 토큰 결과: ' + str(x_test[0]))\n",
    "\n",
    "max_title = -1\n",
    "for i in range(len(x)):\n",
    "    if max_title < len(x[i]):\n",
    "        max_title = len(x[i])\n",
    "       \n",
    "print('학습셋 제목 최대 길이: ' + str(max_title))\n",
    "\n",
    "max_test_title = -1\n",
    "for j in range(len(x_test)):\n",
    "    if max_test_title < len(x_test[j]):\n",
    "        max_test_title = len(x_test[j])\n",
    "               \n",
    "print('테스트셋 제목 최대 길이: ' + str(max_test_title))\n",
    "\n",
    "#패딩\n",
    "padded_x = sequence.pad_sequences(x, maxlen=18)\n",
    "padded_test_x = sequence.pad_sequences(x_test, maxlen=18)\n",
    "\n",
    "print('첫번째 패딩 토큰: ' + str(padded_x[0]))\n",
    "print('첫번째 패딩 토큰 토큰' + str(padded_test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...  7318  4360  4361]\n",
      " [    0     0     0 ...  7319  7320  1168]\n",
      " [    0     0     0 ...  1375  2400  7325]\n",
      " ...\n",
      " [    0     0     0 ... 22908 22909   792]\n",
      " [    0     0     0 ... 22910 22911  5382]\n",
      " [    0     0     0 ...  3104    75   779]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-illness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
